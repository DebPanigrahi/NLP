{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.model import fit\n",
    "from fastai.dataset import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs,bptt = 64,70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH='/home/paperspace/data/arxiv/'\n",
    "\n",
    "df_mb = pd.read_csv(f'{PATH}arxiv.csv')\n",
    "#df_all = pd.read_pickle(f'{PATH}all_arxiv.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27188"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_txt(df):\n",
    "    return '<CAT> ' + df.category.str.replace(r'[\\.\\-]','') + ' <SUMM> ' + df.summary + ' <TITLE> ' + df.title\n",
    "df_mb['txt'] = get_txt(df_mb)\n",
    "#df_all['txt'] = get_txt(df_all)\n",
    "df_all= df_mb\n",
    "n=len(df_all); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}trn/yes', exist_ok=True)\n",
    "os.makedirs(f'{PATH}val/yes', exist_ok=True)\n",
    "os.makedirs(f'{PATH}trn/no', exist_ok=True)\n",
    "os.makedirs(f'{PATH}val/no', exist_ok=True)\n",
    "os.makedirs(f'{PATH}all/trn', exist_ok=True)\n",
    "os.makedirs(f'{PATH}all/val', exist_ok=True)\n",
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (i,(_,r)) in enumerate(df_all.iterrows()):\n",
    "    dset = 'trn' if random.random()>0.1 else 'val'\n",
    "    open(f'{PATH}all/{dset}/{i}.txt', 'w').write(r['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (i,(_,r)) in enumerate(df_mb.iterrows()):\n",
    "    lbl = 'yes' if r.tweeted else 'no'\n",
    "    dset = 'trn' if random.random()>0.1 else 'val'\n",
    "    open(f'{PATH}{dset}/{lbl}/{i}.txt', 'w').write(r['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "\n",
    "my_tok.tokenizer.add_special_case('<SUMM>', [{ORTH: '<SUMM>'}])\n",
    "my_tok.tokenizer.add_special_case('<CAT>', [{ORTH: '<CAT>'}])\n",
    "my_tok.tokenizer.add_special_case('<TITLE>', [{ORTH: '<TITLE>'}])\n",
    "my_tok.tokenizer.add_special_case('<BR />', [{ORTH: '<BR />'}])\n",
    "my_tok.tokenizer.add_special_case('<BR>', [{ORTH: '<BR>'}])\n",
    "\n",
    "def my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=my_spacy_tok)\n",
    "FILES = dict(train='trn', validation='val', test='val')\n",
    "md = LanguageModelData.from_text_files(f'{PATH}all/', TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)\n",
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 13463, 1, 4907202)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', 'of', '-', 'and', 'a', 'to', 'in', 'we']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cat> cssi <summ> evaluating information accuracy in social media is an increasingly important and well - studied area , but limited research has compared journalist - sourced accuracy assessments to their crowdsourced counterparts . this paper demonstrates the differences between these two populations by comparing the features used to predict accuracy assessments in two twitter data sets : credbank and pheme . while our findings are consistent with existing results on feature importance , we develop models that outperform past research . we also show limited overlap exists between the features used by journalists and crowdsourced assessors , and the resulting models poorly predict each other but produce statistically correlated results . this correlation suggests crowdsourced workers are assessing a different aspect of these stories than their journalist counterparts , but these two aspects are linked in a significant way . these differences may be explained by contrasting factual with'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(md.trn_ds[0].text[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "em_sz = 200\n",
    "nh = 500\n",
    "nl = 3\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "    dropout=0.05, dropouth=0.1, dropouti=0.05, dropoute=0.02, wdrop=0.2)\n",
    "# dropout=0.4, dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5\n",
    "#                dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506e759fb7da4a6f8d873cc9c6b557c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       4.69862  4.57728]                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb26f95e5b947c58ca659ab5f19ed3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       4.41557  4.34256]                                  \n",
      "[ 1.       4.317    4.21332]                                  \n",
      "[ 2.       4.18342  4.14482]                                  \n",
      "[ 3.       4.21544  4.12514]                                  \n",
      "[ 4.       4.09785  4.04528]                                  \n",
      "[ 5.       4.01368  3.99439]                                  \n",
      "[ 6.       3.96694  3.98255]                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('adam2_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a793b174b042c2b1baed569b50e1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       4.09782  4.03636]                                  \n",
      "[ 1.       4.02432  3.98748]                                  \n",
      "[ 2.       3.94617  3.9455 ]                                  \n",
      "[ 3.       3.89137  3.91993]                                  \n",
      "[ 4.       3.85898  3.91465]                                  \n",
      "[ 5.       4.01525  3.97096]                                  \n",
      "[ 6.       3.95356  3.93763]                                  \n",
      "[ 7.       3.87995  3.90522]                                  \n",
      "[ 8.       3.83657  3.88224]                                  \n",
      "[ 9.       3.80372  3.87534]                                  \n",
      "[ 10.        3.96715   3.93785]                               \n",
      "[ 11.        3.9186    3.91091]                               \n",
      "[ 12.        3.8476    3.87901]                               \n",
      "[ 13.        3.81081   3.85719]                               \n",
      "[ 14.        3.75302   3.85447]                               \n",
      "[ 15.        3.93298   3.91325]                               \n",
      "[ 16.        3.87447   3.88817]                               \n",
      "[ 17.        3.81578   3.86347]                               \n",
      "[ 18.        3.75947   3.83909]                               \n",
      "[ 19.        3.71743   3.83722]                               \n",
      "[ 20.        3.90653   3.89422]                               \n",
      "[ 21.        3.84783   3.87279]                               \n",
      "[ 22.        3.77793   3.85105]                               \n",
      "[ 23.        3.73614   3.82947]                               \n",
      "[ 24.        3.69329   3.82796]                               \n",
      "[ 25.        3.88515   3.88608]                               \n",
      "[ 26.        3.86532   3.86718]                               \n",
      "[ 27.        3.76758   3.83836]                               \n",
      "[ 28.        3.70089   3.81844]                               \n",
      "[ 29.        3.68203   3.8202 ]                               \n",
      "[ 30.        3.86396   3.87575]                               \n",
      "[ 31.        3.81551   3.85649]                               \n",
      "[ 32.        3.77177   3.83112]                               \n",
      "[ 33.        3.72003   3.81092]                               \n",
      "[ 34.        3.66129   3.81261]                               \n",
      "[ 35.        3.85676   3.86769]                               \n",
      "[ 36.        3.80432   3.85171]                               \n",
      "[ 37.        3.74161   3.82725]                               \n",
      "[ 38.        3.70284   3.80965]                               \n",
      "[ 39.        3.64746   3.80942]                               \n",
      "[ 40.        3.8442    3.86447]                               \n",
      "[ 41.        3.81179   3.84723]                               \n",
      "[ 42.        3.75285   3.82052]                               \n",
      "[ 43.       3.6777   3.8027]                                  \n",
      "[ 44.        3.63957   3.80506]                               \n",
      "[ 45.        3.83761   3.86201]                               \n",
      "[ 46.       3.804    3.8405]                                  \n",
      "[ 47.        3.71821   3.81465]                               \n",
      "[ 48.        3.67544   3.79916]                               \n",
      "[ 49.        3.63726   3.7995 ]                               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 10, wds=1e-6, cycle_len=5, cycle_save_name='adam3_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('adam3_10_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7a5bf6bc364588afae2c8d2d5c0f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       3.80733  3.8631 ]                                  \n",
      "[ 1.       3.81863  3.85936]                                  \n",
      "[ 2.       3.77115  3.84595]                                  \n",
      "[ 3.       3.73258  3.83297]                                  \n",
      "[ 4.       3.70692  3.82077]                                  \n",
      "[ 5.       3.68245  3.80733]                                  \n",
      "[ 6.       3.63998  3.7993 ]                                  \n",
      "[ 7.       3.62947  3.79266]                                  \n",
      "[ 8.       3.66046  3.78922]                                  \n",
      "[ 9.       3.56934  3.79175]                                  \n",
      "[ 10.        3.80811   3.85339]                               \n",
      "[ 11.        3.78514   3.84789]                               \n",
      "[ 12.        3.76451   3.83727]                               \n",
      "[ 13.        3.75033   3.82611]                               \n",
      "[ 14.        3.69643   3.81233]                               \n",
      "[ 15.        3.64555   3.80331]                               \n",
      "[ 16.        3.61369   3.79654]                               \n",
      "[ 17.        3.60508   3.79052]                               \n",
      "[ 18.        3.56731   3.7883 ]                               \n",
      "[ 19.        3.57537   3.78961]                               \n",
      "[ 20.        3.79311   3.8449 ]                               \n",
      "[ 21.        3.77288   3.84413]                               \n",
      "[ 22.        3.7408    3.83145]                               \n",
      "[ 23.        3.71827   3.82335]                               \n",
      "[ 24.        3.67528   3.80895]                               \n",
      "[ 25.        3.64571   3.79717]                               \n",
      "[ 26.        3.63763   3.78857]                               \n",
      "[ 27.        3.60604   3.78429]                               \n",
      "[ 28.        3.60056   3.78254]                               \n",
      "[ 29.        3.57039   3.78615]                               \n",
      "[ 30.        3.78898   3.84256]                               \n",
      "[ 31.        3.76629   3.83833]                               \n",
      "[ 32.        3.73533   3.83052]                               \n",
      "[ 33.        3.70121   3.81678]                               \n",
      "[ 34.        3.68337   3.80434]                               \n",
      "[ 35.        3.6291    3.79505]                               \n",
      "[ 36.        3.60354   3.7859 ]                               \n",
      "[ 37.        3.57626   3.78169]                               \n",
      "[ 38.        3.54406   3.77966]                               \n",
      "[ 39.        3.5381    3.78051]                               \n",
      "[ 40.        3.77212   3.83807]                               \n",
      "[ 41.        3.77366   3.83259]                               \n",
      "[ 42.        3.73914   3.82325]                               \n",
      "[ 43.        3.68977   3.81172]                               \n",
      "[ 44.        3.65527   3.80252]                               \n",
      "[ 45.        3.66108   3.78792]                               \n",
      "[ 46.        3.58178   3.78572]                               \n",
      "[ 47.        3.55123   3.77988]                               \n",
      "[ 48.        3.56663   3.77642]                               \n",
      "[ 49.        3.52118   3.7755 ]                               \n",
      "[ 50.        3.75855   3.83242]                               \n",
      "[ 51.        3.74955   3.83045]                               \n",
      "[ 52.        3.72242   3.82341]                               \n",
      "[ 53.        3.71855   3.81054]                               \n",
      "[ 54.        3.65878   3.8005 ]                               \n",
      "[ 55.        3.65457   3.78823]                               \n",
      "[ 56.        3.57415   3.78095]                               \n",
      "[ 57.        3.63197   3.77228]                               \n",
      "[ 58.        3.53107   3.77556]                               \n",
      "[ 59.        3.51392   3.77587]                               \n",
      "[ 60.        3.76207   3.83094]                               \n",
      "[ 61.        3.73879   3.82518]                               \n",
      "[ 62.        3.71173   3.81732]                               \n",
      "[ 63.        3.67794   3.80982]                               \n",
      "[ 64.        3.64269   3.79993]                               \n",
      "[ 65.        3.60534   3.78854]                               \n",
      "[ 66.        3.59582   3.78027]                               \n",
      "[ 67.        3.55831   3.77593]                               \n",
      "[ 68.        3.52492   3.77202]                               \n",
      "[ 69.        3.52394   3.77441]                               \n",
      "[ 70.        3.74809   3.82793]                               \n",
      "[ 71.        3.72944   3.82611]                               \n",
      "[ 72.        3.71848   3.81735]                               \n",
      "[ 73.        3.67205   3.80852]                               \n",
      "[ 74.        3.63256   3.79564]                               \n",
      "[ 75.        3.60316   3.78749]                               \n",
      "[ 76.        3.57446   3.77638]                               \n",
      "[ 77.        3.55343   3.77297]                               \n",
      "[ 78.        3.50373   3.77084]                               \n",
      "[ 79.        3.50184   3.77226]                               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 8, wds=1e-6, cycle_len=10, cycle_save_name='adam3_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8021ef2d5854b60bd17b408e4593fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       3.51261  3.77196]                                  \n",
      "[ 1.       3.76152  3.82596]                                  \n",
      "[ 2.       3.73532  3.82539]                                  \n",
      "[ 3.       3.71867  3.81948]                                  \n",
      "[ 4.       3.69809  3.8182 ]                                  \n",
      "[ 5.       3.69004  3.81345]                                  \n",
      "[ 6.       3.72027  3.80901]                                  \n",
      "[ 7.       3.64732  3.80159]                                  \n",
      "[ 8.       3.6305   3.79722]                                  \n",
      "[ 9.       3.60785  3.79271]                                  \n",
      "[ 10.        3.59686   3.78879]                               \n",
      "[ 11.        3.5682    3.78272]                               \n",
      "[ 12.        3.54136   3.77841]                               \n",
      "[ 13.        3.53534   3.77319]                               \n",
      "[ 14.        3.54598   3.7677 ]                               \n",
      "[ 15.        3.53794   3.76736]                               \n",
      "[ 16.        3.50415   3.76986]                               \n",
      "[ 17.        3.4784    3.76678]                               \n",
      "[ 18.        3.49786   3.76623]                               \n",
      "[ 19.        3.46774   3.77148]                               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('adam3_20_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learner.save('adam3_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_str(s): return TEXT.preprocess(TEXT.tokenize(s))\n",
    "def num_str(s): return TEXT.numericalize([proc_str(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m=learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=\"\"\"<CAT> cscv <SUMM> algorithms that\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_model(m, s, l=50):\n",
    "    t = num_str(s)\n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(l):\n",
    "        n=res[-1].topk(2)[1]\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        word = TEXT.vocab.itos[n.data[0]]\n",
    "        print(word, end=' ')\n",
    "        if word=='<eos>': break\n",
    "        res,*_ = m(n[0].unsqueeze(0))\n",
    "\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...use the internet of things ( iot ) to provide a reliable and reliable communication infrastructure are a key component of the future wireless networks . in this paper , we propose a novel approach to the problem of wireless sensor networks ( wsn ) . the proposed approach is "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> csni <SUMM> algorithms that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...use a single image to generate a set of images are often used to generate images with high quality . however , the quality of the generated images is not well understood . in this paper , we propose a novel method for image denoising that is based on the "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> cscv <SUMM> algorithms that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...the use of deep learning for image classification <eos> "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> on \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...the performance of wireless networks <eos> "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> on \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...a new approach for image segmentation <eos> "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> towards \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...a new approach to the problem of network design <eos> "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> towards \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bs,bptt = 64,70\n",
    "PATH='/home/paperspace/data/arxiv/'\n",
    "df_mb = pd.read_csv(f'{PATH}arxiv.csv')\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "\n",
    "my_tok.tokenizer.add_special_case('<SUMM>', [{ORTH: '<SUMM>'}])\n",
    "my_tok.tokenizer.add_special_case('<CAT>', [{ORTH: '<CAT>'}])\n",
    "my_tok.tokenizer.add_special_case('<TITLE>', [{ORTH: '<TITLE>'}])\n",
    "my_tok.tokenizer.add_special_case('<BR />', [{ORTH: '<BR />'}])\n",
    "my_tok.tokenizer.add_special_case('<BR>', [{ORTH: '<BR>'}])\n",
    "\n",
    "def my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]\n",
    "\n",
    "def get_txt(df):\n",
    "    return '<CAT> ' + df.category.str.replace(r'[\\.\\-]','') + ' <SUMM> ' + df.summary + ' <TITLE> ' + df.title\n",
    "\n",
    "df_mb['txt'] = get_txt(df_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mb['target'] = (np.random.rand(len(df_mb))*100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>time</th>\n",
       "      <th>favorites</th>\n",
       "      <th>rts</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>tweeted</th>\n",
       "      <th>txt</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arxiv.org/abs/1611.10003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Tom A. F. Anderson, C. -H. Ruan]</td>\n",
       "      <td>q-bio.NC</td>\n",
       "      <td>2016-11-30 05:17:11</td>\n",
       "      <td>In summary of the research findings presented ...</td>\n",
       "      <td>Vocabulary and the Brain: Evidence from Neuroi...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;CAT&gt; qbioNC &lt;SUMM&gt; In summary of the research...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arxiv.org/abs/1611.10007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[M. Amin Rahimian, Amir G. Aghdam]</td>\n",
       "      <td>cs.SY</td>\n",
       "      <td>2016-11-30 05:37:11</td>\n",
       "      <td>In this paper, structural controllability of a...</td>\n",
       "      <td>Structural Controllability of Multi-Agent Netw...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;CAT&gt; csSY &lt;SUMM&gt; In this paper, structural co...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arxiv.org/abs/1611.10010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Debidatta Dwibedi, Tomasz Malisiewicz, Vijay ...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2016-11-30 06:00:47</td>\n",
       "      <td>We present a Deep Cuboid Detector which takes ...</td>\n",
       "      <td>Deep Cuboid Detection: Beyond 2D Bounding Boxes</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;CAT&gt; csCV &lt;SUMM&gt; We present a Deep Cuboid Det...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       link time  favorites  rts  \\\n",
       "0  arxiv.org/abs/1611.10003  NaN        NaN  NaN   \n",
       "1  arxiv.org/abs/1611.10007  NaN        NaN  NaN   \n",
       "2  arxiv.org/abs/1611.10010  NaN        NaN  NaN   \n",
       "\n",
       "                                             authors  category  \\\n",
       "0                  [Tom A. F. Anderson, C. -H. Ruan]  q-bio.NC   \n",
       "1                 [M. Amin Rahimian, Amir G. Aghdam]     cs.SY   \n",
       "2  [Debidatta Dwibedi, Tomasz Malisiewicz, Vijay ...     cs.CV   \n",
       "\n",
       "             published                                            summary  \\\n",
       "0  2016-11-30 05:17:11  In summary of the research findings presented ...   \n",
       "1  2016-11-30 05:37:11  In this paper, structural controllability of a...   \n",
       "2  2016-11-30 06:00:47  We present a Deep Cuboid Detector which takes ...   \n",
       "\n",
       "                                               title  tweeted  \\\n",
       "0  Vocabulary and the Brain: Evidence from Neuroi...        0   \n",
       "1  Structural Controllability of Multi-Agent Netw...        0   \n",
       "2    Deep Cuboid Detection: Beyond 2D Bounding Boxes        0   \n",
       "\n",
       "                                                 txt  target  \n",
       "0  <CAT> qbioNC <SUMM> In summary of the research...      57  \n",
       "1  <CAT> csSY <SUMM> In this paper, structural co...      78  \n",
       "2  <CAT> csCV <SUMM> We present a Deep Cuboid Det...      98  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mb.loc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArxivDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for label in ['yes', 'no']:\n",
    "            for fname in iglob(os.path.join(path, label, '*.txt')):\n",
    "                with open(fname, 'r') as f: text = f.readline()\n",
    "                # adds label and text to fields variables TEXT, ARX_LABEL    \n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.data',\n",
    "               train='train', test='test', **kwargs):\n",
    "        return super().splits(\n",
    "            root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ARX_LABEL = data.Field(sequential=False)\n",
    "splits = ArxivDataset.splits(TEXT, ARX_LABEL, PATH, train='trn', test='val')\n",
    "train_ds, val_ds = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating splits from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredNLPDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, split_type, text_field, cat_field, target_field, dfs, **kwargs):\n",
    "        fields = [(\"text\", text_field), (\"category\", cat_field), (\"target\", target_field) ]\n",
    "        examples = []\n",
    "        \n",
    "        df = dfs[split_type]\n",
    "        dependent = 'target'\n",
    "        \n",
    "        for i,row in df.iterrows():\n",
    "            text = row['txt']\n",
    "            cat = row['category']\n",
    "            label = None\n",
    "            if dependent in dfs[split_type]:\n",
    "                label = row[dependent]\n",
    "            examples.append(data.Example.fromlist([text, cat, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, cat_field, text_field, target_field, name_as_path,\n",
    "               train, val, test, dfs, **kwargs):\n",
    "        return super().splits(name_as_path, \n",
    "            text_field=text_field, cat_field=cat_field, target_field=target_field, \n",
    "            train=train, validation=val, test=test, dfs=dfs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = df_mb.iloc[:25000], df_mb.iloc[25001:]\n",
    "dfs = {'train': df_train, 'val': df_val, 'test': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??data.Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = data.Field(sequential=False)\n",
    "ARX_TARGET = data.Field(sequential=False, tensor_type=torch.DoubleTensor, use_vocab=False)\n",
    "# path is now the name\n",
    "splits = StructuredNLPDataset.splits(CATEGORY, TEXT, ARX_TARGET, '', train='train', val='val', test=None, dfs=dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??torchtext.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cs.SE',\n",
       " 17,\n",
       " ['<cat>',\n",
       "  'csse',\n",
       "  '<summ>',\n",
       "  'background',\n",
       "  ':',\n",
       "  'while',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  'programs',\n",
       "  'are',\n",
       "  'not',\n",
       "  'new',\n",
       "  'in',\n",
       "  'software',\n",
       "  'development',\n",
       "  ',',\n",
       "  'an',\n",
       "  '\\n',\n",
       "  'increasing',\n",
       "  'number',\n",
       "  'of',\n",
       "  'companies',\n",
       "  ',',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'open',\n",
       "  'source',\n",
       "  'projects',\n",
       "  ',',\n",
       "  'rely',\n",
       "  'on',\n",
       "  '\\n',\n",
       "  'external',\n",
       "  'parties',\n",
       "  'to',\n",
       "  'perform',\n",
       "  'the',\n",
       "  'security',\n",
       "  'assessment',\n",
       "  'of',\n",
       "  'their',\n",
       "  'software',\n",
       "  'for',\n",
       "  '\\n',\n",
       "  'reward',\n",
       "  '.',\n",
       "  'however',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'relatively',\n",
       "  'little',\n",
       "  'empirical',\n",
       "  'knowledge',\n",
       "  'about',\n",
       "  'the',\n",
       "  '\\n',\n",
       "  'characteristics',\n",
       "  'of',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  'program',\n",
       "  'contributors',\n",
       "  '.',\n",
       "  'aim',\n",
       "  ':',\n",
       "  'this',\n",
       "  'paper',\n",
       "  'aims',\n",
       "  'to',\n",
       "  '\\n',\n",
       "  'understand',\n",
       "  'those',\n",
       "  'contributors',\n",
       "  'by',\n",
       "  'highlighting',\n",
       "  'the',\n",
       "  'heterogeneity',\n",
       "  'among',\n",
       "  'them',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'method',\n",
       "  ':',\n",
       "  'we',\n",
       "  'analyzed',\n",
       "  'the',\n",
       "  'histories',\n",
       "  'of',\n",
       "  '82',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  'programs',\n",
       "  'and',\n",
       "  '2,504',\n",
       "  'distinct',\n",
       "  '\\n',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  'contributors',\n",
       "  ',',\n",
       "  'and',\n",
       "  'conducted',\n",
       "  'a',\n",
       "  'quantitative',\n",
       "  'and',\n",
       "  'qualitative',\n",
       "  'survey',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'results',\n",
       "  ':',\n",
       "  'we',\n",
       "  'found',\n",
       "  'that',\n",
       "  'there',\n",
       "  'are',\n",
       "  'project',\n",
       "  '-',\n",
       "  'specific',\n",
       "  'and',\n",
       "  'non',\n",
       "  '-',\n",
       "  'specific',\n",
       "  'contributors',\n",
       "  '\\n',\n",
       "  'who',\n",
       "  'have',\n",
       "  'different',\n",
       "  'motivations',\n",
       "  'for',\n",
       "  'contributing',\n",
       "  'to',\n",
       "  'the',\n",
       "  'products',\n",
       "  'and',\n",
       "  '\\n',\n",
       "  'organizations',\n",
       "  '.',\n",
       "  'conclusions',\n",
       "  ':',\n",
       "  'our',\n",
       "  'findings',\n",
       "  'provide',\n",
       "  'insights',\n",
       "  'to',\n",
       "  'make',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  '\\n',\n",
       "  'programs',\n",
       "  'better',\n",
       "  'and',\n",
       "  'for',\n",
       "  'further',\n",
       "  'studies',\n",
       "  'of',\n",
       "  'new',\n",
       "  'software',\n",
       "  'development',\n",
       "  'roles',\n",
       "  '.',\n",
       "  '<title>',\n",
       "  'understanding',\n",
       "  'the',\n",
       "  'heterogeneity',\n",
       "  'of',\n",
       "  'contributors',\n",
       "  'in',\n",
       "  'bug',\n",
       "  'bounty',\n",
       "  'programs'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].examples[0].category, splits[1].examples[0].target, splits[1].examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataLoader():\n",
    "    def __init__(self, src, txt_fld, cat_fld, y_fld):\n",
    "        self.src,self.txt_fld,self.cat_fld,self.y_fld = src,txt_fld,cat_fld,y_fld\n",
    "\n",
    "    def __len__(self): return len(self.src)-1\n",
    "\n",
    "    def __iter__(self):\n",
    "        it = iter(self.src)\n",
    "        for i in range(len(self)):\n",
    "            b = next(it)\n",
    "            yield getattr(b, self.txt_fld), getattr(b, self.cat_fld), getattr(b, self.y_fld)\n",
    "\n",
    "\n",
    "class TextData(ModelData):\n",
    "    def create_td(self, it): return TextDataLoader(it, self.text_fld, self.label_fld)\n",
    "\n",
    "    @classmethod\n",
    "    def from_splits(cls, path, splits, bs, text_name='text', label_name='label', cat_name='category'):\n",
    "        text_fld = splits[0].fields[text_name]\n",
    "        label_fld = splits[0].fields[label_name]\n",
    "        cat_fld = splits[0].fields[cat_name]\n",
    "        cat_fld.build_vocab(splits[0])\n",
    "        # label_fld.build_vocab(splits[0])\n",
    "        iters = torchtext.data.BucketIterator.splits(splits, batch_size=bs)\n",
    "        trn_iter,val_iter,test_iter = iters[0],iters[1],None\n",
    "        test_dl = None\n",
    "        if len(iters) == 3:\n",
    "            test_iter = iters[2]\n",
    "            test_dl = TextDataLoader(test_iter, text_name, label_name)\n",
    "        trn_dl = TextDataLoader(trn_iter, text_name, cat_name, label_name)\n",
    "        val_dl = TextDataLoader(val_iter, text_name, cat_name, label_name)\n",
    "        obj = cls.from_dls(path, trn_dl, val_dl, test_dl)\n",
    "        obj.bs = bs\n",
    "        obj.pad_idx = text_fld.vocab.stoi[text_fld.pad_token]\n",
    "        obj.nt = len(text_fld.vocab)\n",
    "        #obj.c = len(label_fld.vocab)\n",
    "        return obj\n",
    "\n",
    "    def get_model(self, opt_fn, max_sl, bptt, emb_sz, n_hid, n_layers, **kwargs):\n",
    "        m = get_rnn_classifer(max_sl, bptt, self.bs, self.c, self.nt, emb_sz=emb_sz, n_hid=n_hid, n_layers=n_layers,\n",
    "                              pad_token=self.pad_idx, **kwargs)\n",
    "        model = TextModel(to_gpu(m))\n",
    "        return RNN_Learner(self, model, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "md2 = TextData.from_splits(PATH, splits, bs, label_name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = next(iter(md2.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "     24     24     24  ...      24     24     24\n",
       "    140   1838    392  ...     140   1637   1566\n",
       "     26     26     26  ...      26     26     26\n",
       "         ...            â‹±           ...         \n",
       "    142   1444   1824  ...    1835      6     10\n",
       "      1      1      1  ...      82    305    216\n",
       "      1      1      1  ...   10558    142    770\n",
       " [torch.cuda.LongTensor of size 173x64 (GPU 0)], Variable containing:\n",
       "    1\n",
       "   21\n",
       "    4\n",
       "    6\n",
       "    5\n",
       "    6\n",
       "    9\n",
       "    2\n",
       "   15\n",
       "    5\n",
       "    3\n",
       "   44\n",
       "   38\n",
       "    8\n",
       "    6\n",
       "   10\n",
       "    5\n",
       "    2\n",
       "   50\n",
       "    4\n",
       "    4\n",
       "    4\n",
       "   15\n",
       "    1\n",
       "   42\n",
       "   31\n",
       "   28\n",
       "   63\n",
       "   22\n",
       "    2\n",
       "   31\n",
       "   52\n",
       "   55\n",
       "    2\n",
       "   31\n",
       "    8\n",
       "    8\n",
       "    1\n",
       "    4\n",
       "    2\n",
       "   12\n",
       "    4\n",
       "    2\n",
       "    5\n",
       "    1\n",
       "   20\n",
       "    5\n",
       "    4\n",
       "    2\n",
       "    2\n",
       "   10\n",
       "   10\n",
       "   43\n",
       "    1\n",
       "   12\n",
       "    1\n",
       "    1\n",
       "  111\n",
       "    1\n",
       "   26\n",
       "    3\n",
       "    1\n",
       "   17\n",
       "   16\n",
       " [torch.cuda.LongTensor of size 64 (GPU 0)], Variable containing:\n",
       "  42\n",
       "  21\n",
       "  60\n",
       "  48\n",
       "   3\n",
       "  39\n",
       "  60\n",
       "  25\n",
       "  69\n",
       "  15\n",
       "  56\n",
       "  46\n",
       "  19\n",
       "  92\n",
       "  54\n",
       "  55\n",
       "   2\n",
       "  61\n",
       "  60\n",
       "  84\n",
       "  31\n",
       "  96\n",
       "  79\n",
       "  19\n",
       "  14\n",
       "  44\n",
       "  18\n",
       "  82\n",
       "  54\n",
       "  31\n",
       "  89\n",
       "  59\n",
       "  20\n",
       "  56\n",
       "  98\n",
       "  93\n",
       "   7\n",
       "  58\n",
       "  75\n",
       "  48\n",
       "  97\n",
       "  50\n",
       "  11\n",
       "  50\n",
       "  72\n",
       "  86\n",
       "  15\n",
       "  62\n",
       "  33\n",
       "  58\n",
       "  18\n",
       "  35\n",
       "  23\n",
       "  28\n",
       "  62\n",
       "  21\n",
       "   7\n",
       "   8\n",
       "  44\n",
       "   0\n",
       "  39\n",
       "  60\n",
       "  27\n",
       "  67\n",
       " [torch.cuda.DoubleTensor of size 64 (GPU 0)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??TextDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??data.Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#            dropout=0.3, dropouti=0.4, wdrop=0.3, dropoute=0.05, dropouth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prec_at_6(preds,targs):\n",
    "    precision, recall, _ = precision_recall_curve(targs==2, preds[:,2])\n",
    "    print(recall[precision>=0.6][0])\n",
    "    return recall[precision>=0.6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout=0.4, dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5\n",
    "m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, \n",
    "           dropout=0.1, dropouti=0.65, wdrop=0.5, dropoute=0.1, dropouth=0.3)\n",
    "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m3.clip=25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN (\n",
       "  (0): MultiBatchRNN (\n",
       "    (encoder): Embedding(13463, 200, padding_idx=1)\n",
       "    (rnns): ModuleList (\n",
       "      (0): WeightDrop (\n",
       "        (module): LSTM(200, 500, dropout=0.3)\n",
       "      )\n",
       "      (1): WeightDrop (\n",
       "        (module): LSTM(500, 500, dropout=0.3)\n",
       "      )\n",
       "      (2): WeightDrop (\n",
       "        (module): LSTM(500, 200, dropout=0.3)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout (\n",
       "    )\n",
       "    (dropouth): LockedDropout (\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier (\n",
       "    (decoder): Linear (600 -> 3)\n",
       "    (dropout): LockedDropout (\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m3.load_encoder(f'adam3_20_enc')\n",
    "lrs=np.array([1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN (\n",
       "  (0): MultiBatchRNN (\n",
       "    (encoder): Embedding(13463, 200, padding_idx=1)\n",
       "    (rnns): ModuleList (\n",
       "      (0): WeightDrop (\n",
       "        (module): LSTM(200, 500, dropout=0.3)\n",
       "      )\n",
       "      (1): WeightDrop (\n",
       "        (module): LSTM(500, 500, dropout=0.3)\n",
       "      )\n",
       "      (2): WeightDrop (\n",
       "        (module): LSTM(500, 200, dropout=0.3)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout (\n",
       "    )\n",
       "    (dropouth): LockedDropout (\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier (\n",
       "    (decoder): Linear (600 -> 3)\n",
       "    (dropout): LockedDropout (\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f0382ae296471f894201fa39433cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.48448  0.38832  0.8151 ]                        \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7285e5a43049d9b96210423d254eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.46593  0.42034  0.7939 ]                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m3.freeze_to(-1)\n",
    "m3.fit(lrs/2, 1, metrics=[accuracy])\n",
    "m3.unfreeze()\n",
    "m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368982c7b03f4e5582ac427f1bf0bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.43938  0.39489  0.7939 ]                        \n",
      "[ 1.       0.41452  0.40475  0.78311]                        \n",
      "[ 2.       0.40601  0.38734  0.79576]                        \n",
      "[ 3.       0.41125  0.39618  0.79018]                        \n",
      "[ 4.       0.40869  0.37752  0.80655]                        \n",
      "[ 5.       0.40786  0.36441  0.81994]                        \n",
      "[ 6.       0.39978  0.37931  0.80059]                        \n",
      "[ 7.       0.39096  0.3716   0.81138]                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m3.fit(lrs, 2, metrics=[accuracy], cycle_len=4, cycle_save_name='imdb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5357142857142857"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_at_6(*m3.predict_with_targs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf5c47e8a7a419dabcc7c81b2654525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       0.39376  0.36864  0.81808]                        \n",
      "[ 1.       0.38753  0.36827  0.81808]                        \n",
      "[ 2.       0.39997  0.36362  0.8192 ]                        \n",
      "[ 3.       0.39627  0.36413  0.82143]                        \n",
      "[ 4.       0.37498  0.36207  0.82031]                        \n",
      "[ 5.       0.37599  0.36418  0.81994]                        \n",
      "[ 6.       0.38193  0.36238  0.82329]                        \n",
      "[ 7.       0.3761   0.36019  0.82552]                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m3.fit(lrs, 4, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598639455782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59863945578231292"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_at_6(*m3.predict_with_targs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
